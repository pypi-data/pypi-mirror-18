THEANO_FLAGS=floatX=float32,device=gpu,nvcc.fastmath=True
/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2016-11-29 21:59:40,836 train: TRAINING OPTIONS
2016-11-29 21:59:40,836 train: sequence_length: 25
2016-11-29 21:59:40,836 train: patience: 0
2016-11-29 21:59:40,836 train: batch_size: 32
2016-11-29 21:59:40,837 train: min_epochs: 1
2016-11-29 21:59:40,837 train: validation_frequency: 1
2016-11-29 21:59:40,837 train: max_epochs: 15
2016-11-29 21:59:40,837 train: max_annealing_count: 0
2016-11-29 21:59:40,837 train: stopping_criterion: no-improvement
2016-11-29 21:59:40,837 train: OPTIMIZATION OPTIONS
2016-11-29 21:59:40,837 train: sqr_gradient_decay_rate: 0.999
2016-11-29 21:59:40,837 train: num_noise_samples: 1
2016-11-29 21:59:40,837 train: epsilon: 1e-06
2016-11-29 21:59:40,837 train: noise_sharing: None
2016-11-29 21:59:40,837 train: learning_rate: 1.0
2016-11-29 21:59:40,837 train: unk_penalty: None
2016-11-29 21:59:40,837 train: cost_function: cross-entropy
2016-11-29 21:59:40,837 train: weights: [ 1.]
2016-11-29 21:59:40,837 train: ignore_unk: False
2016-11-29 21:59:40,837 train: momentum: 0.9
2016-11-29 21:59:40,837 train: max_gradient_norm: 5.0
2016-11-29 21:59:40,837 train: gradient_decay_rate: 0.9
2016-11-29 21:59:40,838 train: method: adagrad
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2016-11-29 21:59:41,990 __init__: One epoch of training data contains 1778 mini-batch updates.
2016-11-29 21:59:41,991 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2016-11-29 21:59:41,991 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2016-11-29 21:59:42,014 _reset: Generating a random order of input lines.
Building neural network.
2016-11-29 21:59:42,021 __init__: Creating layers.
2016-11-29 21:59:42,021 __init__: - NetworkInput name=word_input inputs=[] size=10001
2016-11-29 21:59:42,021 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100
2016-11-29 21:59:42,060 __init__: - LSTMLayer name=hidden_layer inputs=[projection_layer] size=256
2016-11-29 21:59:42,180 __init__: - HSoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001
2016-11-29 21:59:42,180 __init__:   level1_size=101 level2_size=100
2016-11-29 21:59:42,302 __init__: Initializing parameters.
2016-11-29 21:59:42,302 __init__: - layers/projection_layer/W size=1000100 type=float32
2016-11-29 21:59:42,303 __init__: - layers/hidden_layer/layer_input/W size=102400 type=float32
2016-11-29 21:59:42,303 __init__: - layers/hidden_layer/step_input/W size=262144 type=float32
2016-11-29 21:59:42,303 __init__: - layers/hidden_layer/layer_input/b size=1024 type=float32
2016-11-29 21:59:42,303 __init__: - layers/output_layer/input/W size=25856 type=float32
2016-11-29 21:59:42,303 __init__: - layers/output_layer/input/b size=101 type=float32
2016-11-29 21:59:42,303 __init__: - layers/output_layer/level1/W size=2585600 type=float32
2016-11-29 21:59:42,303 __init__: - layers/output_layer/level1/b size=10100 type=float32
2016-11-29 21:59:42,303 __init__: Total number of parameters: 3987325
Compiling optimization function.
Building text scorer for cross-validation.
Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2016-11-29 22:00:07,989 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 6.01, duration = 7.7 ms
2016-11-29 22:00:23,305 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.87, duration = 7.6 ms
2016-11-29 22:00:38,623 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.37, duration = 7.6 ms
2016-11-29 22:00:53,944 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.38, duration = 7.6 ms
2016-11-29 22:01:09,259 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.61, duration = 7.6 ms
2016-11-29 22:01:24,574 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.27, duration = 7.6 ms
2016-11-29 22:01:39,891 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.55, duration = 7.6 ms
2016-11-29 22:01:55,212 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 5.09, duration = 7.6 ms
2016-11-29 22:02:10,675 _validate: [1772] First validation sample, perplexity 169.72.
2016-11-29 22:02:17,769 _validate: [1775] Center of validation, perplexity 170.13.
2016-11-29 22:02:24,960 _validate: [1778] Last validation sample, perplexity 169.61.
2016-11-29 22:02:24,999 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 22:02:25,000 _log_validation: [1778] Validation set cost history: [169.7]
2016-11-29 22:02:25,001 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 2.5 minutes. Best validation perplexity 169.72.
2016-11-29 22:02:26,693 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 5.05, duration = 7.6 ms
2016-11-29 22:02:42,016 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.29, duration = 7.6 ms
2016-11-29 22:02:57,325 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 5.07, duration = 7.6 ms
2016-11-29 22:03:12,643 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 5.19, duration = 7.6 ms
2016-11-29 22:03:27,945 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 5.18, duration = 7.6 ms
2016-11-29 22:03:43,242 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 5.10, duration = 7.6 ms
2016-11-29 22:03:58,537 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.82, duration = 7.5 ms
2016-11-29 22:04:14,263 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.98, duration = 7.8 ms
2016-11-29 22:04:30,020 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 5.11, duration = 7.8 ms
2016-11-29 22:04:44,121 _validate: [1772] First validation sample, perplexity 141.52.
2016-11-29 22:04:51,192 _validate: [1775] Center of validation, perplexity 141.62.
2016-11-29 22:04:58,293 _validate: [1778] Last validation sample, perplexity 141.58.
2016-11-29 22:04:58,318 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 22:04:58,318 _log_validation: [1778] Validation set cost history: 169.7 [141.6]
2016-11-29 22:04:58,320 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 2.6 minutes. Best validation perplexity 141.58.
2016-11-29 22:05:01,750 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 4.43, duration = 7.6 ms
2016-11-29 22:05:17,031 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 4.88, duration = 7.6 ms
2016-11-29 22:05:32,309 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 4.57, duration = 7.6 ms
2016-11-29 22:05:47,590 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.75, duration = 7.6 ms
2016-11-29 22:06:02,887 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 4.87, duration = 7.6 ms
2016-11-29 22:06:18,176 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 4.80, duration = 7.6 ms
2016-11-29 22:06:33,464 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 4.60, duration = 7.6 ms
2016-11-29 22:06:48,746 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.89, duration = 7.5 ms
2016-11-29 22:07:04,144 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.45, duration = 7.8 ms
2016-11-29 22:07:16,509 _validate: [1772] First validation sample, perplexity 134.38.
2016-11-29 22:07:23,585 _validate: [1775] Center of validation, perplexity 134.17.
2016-11-29 22:07:30,682 _validate: [1778] Last validation sample, perplexity 134.28.
2016-11-29 22:07:30,705 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 22:07:30,705 _log_validation: [1778] Validation set cost history: 169.7 141.6 [134.3]
2016-11-29 22:07:30,707 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 2.5 minutes. Best validation perplexity 134.26.
2016-11-29 22:07:35,912 _log_update: [66] (3.7 %) of epoch 4 -- lr = 1, cost = 4.33, duration = 7.8 ms
2016-11-29 22:07:51,675 _log_update: [266] (15.0 %) of epoch 4 -- lr = 1, cost = 4.27, duration = 7.8 ms
2016-11-29 22:08:07,221 _log_update: [466] (26.2 %) of epoch 4 -- lr = 1, cost = 4.64, duration = 7.7 ms
2016-11-29 22:08:22,503 _log_update: [666] (37.5 %) of epoch 4 -- lr = 1, cost = 4.65, duration = 7.5 ms
2016-11-29 22:08:37,776 _log_update: [866] (48.7 %) of epoch 4 -- lr = 1, cost = 4.22, duration = 7.6 ms
2016-11-29 22:08:53,058 _log_update: [1066] (60.0 %) of epoch 4 -- lr = 1, cost = 4.39, duration = 7.6 ms
2016-11-29 22:09:08,349 _log_update: [1266] (71.2 %) of epoch 4 -- lr = 1, cost = 4.39, duration = 7.5 ms
2016-11-29 22:09:23,628 _log_update: [1466] (82.5 %) of epoch 4 -- lr = 1, cost = 4.58, duration = 7.5 ms
2016-11-29 22:09:38,904 _log_update: [1666] (93.7 %) of epoch 4 -- lr = 1, cost = 4.42, duration = 7.6 ms
2016-11-29 22:09:49,273 _validate: [1772] First validation sample, perplexity 134.98.
2016-11-29 22:09:56,306 _validate: [1775] Center of validation, perplexity 134.82.
2016-11-29 22:10:03,379 _validate: [1778] Last validation sample, perplexity 134.71.
2016-11-29 22:10:03,379 _log_validation: [1778] Validation set cost history: 169.7 141.6 [134.3] 134.8
2016-11-29 22:10:03,379 set_state: layers/output_layer/input/W <- array(256, 101)
2016-11-29 22:10:03,380 set_state: layers/output_layer/level1/b <- array(101, 100)
2016-11-29 22:10:03,380 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 22:10:03,381 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 22:10:03,381 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 22:10:03,383 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 22:10:03,386 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2016-11-29 22:10:03,386 set_state: layers/output_layer/input/b <- array(101,)
2016-11-29 22:10:03,387 _reset_state: [1775] (99.83 %) of epoch 3
2016-11-29 22:10:03,388 _log_validation: [1775] Validation set cost history: 169.7 141.6 [134.3]
2016-11-29 22:10:03,388 set_state: Restored iterator to line 41999 of 42068.
2016-11-29 22:10:03,389 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-11-29 22:10:03,389 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2016-11-29 22:10:03,390 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2016-11-29 22:10:03,391 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2016-11-29 22:10:03,391 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-11-29 22:10:03,392 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2016-11-29 22:10:03,392 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-11-29 22:10:03,393 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2016-11-29 22:10:03,396 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2016-11-29 22:10:03,397 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-11-29 22:10:03,397 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-11-29 22:10:03,398 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2016-11-29 22:10:03,399 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-11-29 22:10:03,399 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-11-29 22:10:03,400 set_state: layers/output_layer/input/b_gradient <- array(101,)
2016-11-29 22:10:03,403 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 3.
2016-11-29 22:10:03,404 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 2.5 minutes. Best validation perplexity 134.26.
2016-11-29 22:10:10,137 _log_update: [88] (4.9 %) of epoch 4 -- lr = 0.5, cost = 4.34, duration = 7.6 ms
2016-11-29 22:10:25,423 _log_update: [288] (16.2 %) of epoch 4 -- lr = 0.5, cost = 4.44, duration = 7.5 ms
2016-11-29 22:10:40,696 _log_update: [488] (27.4 %) of epoch 4 -- lr = 0.5, cost = 4.11, duration = 7.6 ms
2016-11-29 22:10:55,977 _log_update: [688] (38.7 %) of epoch 4 -- lr = 0.5, cost = 4.26, duration = 7.6 ms
2016-11-29 22:11:11,260 _log_update: [888] (49.9 %) of epoch 4 -- lr = 0.5, cost = 4.16, duration = 7.6 ms
2016-11-29 22:11:26,550 _log_update: [1088] (61.2 %) of epoch 4 -- lr = 0.5, cost = 4.04, duration = 7.6 ms
2016-11-29 22:11:41,827 _log_update: [1288] (72.4 %) of epoch 4 -- lr = 0.5, cost = 4.63, duration = 7.5 ms
2016-11-29 22:11:57,105 _log_update: [1488] (83.7 %) of epoch 4 -- lr = 0.5, cost = 4.43, duration = 7.6 ms
2016-11-29 22:12:12,388 _log_update: [1688] (94.9 %) of epoch 4 -- lr = 0.5, cost = 4.23, duration = 7.6 ms
2016-11-29 22:12:21,085 _validate: [1772] First validation sample, perplexity 132.35.
2016-11-29 22:12:28,119 _validate: [1775] Center of validation, perplexity 132.22.
2016-11-29 22:12:35,170 _validate: [1778] Last validation sample, perplexity 132.28.
2016-11-29 22:12:35,192 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 22:12:35,192 _log_validation: [1778] Validation set cost history: 169.7 141.6 134.3 [132.3]
2016-11-29 22:12:35,194 _reset: Generating a random order of input lines.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 132.29.
2016-11-29 22:12:43,594 _log_update: [110] (6.2 %) of epoch 5 -- lr = 0.5, cost = 3.92, duration = 7.6 ms
2016-11-29 22:12:58,868 _log_update: [310] (17.4 %) of epoch 5 -- lr = 0.5, cost = 3.97, duration = 7.6 ms
2016-11-29 22:13:14,150 _log_update: [510] (28.7 %) of epoch 5 -- lr = 0.5, cost = 4.28, duration = 7.6 ms
2016-11-29 22:13:29,418 _log_update: [710] (39.9 %) of epoch 5 -- lr = 0.5, cost = 4.11, duration = 7.6 ms
2016-11-29 22:13:44,690 _log_update: [910] (51.2 %) of epoch 5 -- lr = 0.5, cost = 4.21, duration = 7.6 ms
2016-11-29 22:13:59,967 _log_update: [1110] (62.4 %) of epoch 5 -- lr = 0.5, cost = 4.30, duration = 7.6 ms
2016-11-29 22:14:15,242 _log_update: [1310] (73.7 %) of epoch 5 -- lr = 0.5, cost = 4.14, duration = 7.6 ms
2016-11-29 22:14:30,507 _log_update: [1510] (84.9 %) of epoch 5 -- lr = 0.5, cost = 4.10, duration = 7.6 ms
2016-11-29 22:14:45,780 _log_update: [1710] (96.2 %) of epoch 5 -- lr = 0.5, cost = 4.25, duration = 7.6 ms
2016-11-29 22:14:52,782 _validate: [1772] First validation sample, perplexity 135.69.
2016-11-29 22:14:59,814 _validate: [1775] Center of validation, perplexity 135.55.
2016-11-29 22:15:06,872 _validate: [1778] Last validation sample, perplexity 135.53.
2016-11-29 22:15:06,872 _log_validation: [1778] Validation set cost history: 169.7 141.6 134.3 [132.3] 135.6
2016-11-29 22:15:06,873 set_state: layers/output_layer/input/W <- array(256, 101)
2016-11-29 22:15:06,873 set_state: layers/output_layer/level1/b <- array(101, 100)
2016-11-29 22:15:06,874 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 22:15:06,874 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 22:15:06,874 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 22:15:06,876 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 22:15:06,879 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2016-11-29 22:15:06,879 set_state: layers/output_layer/input/b <- array(101,)
2016-11-29 22:15:06,881 _reset_state: [1775] (99.83 %) of epoch 4
2016-11-29 22:15:06,881 _log_validation: [1775] Validation set cost history: 169.7 141.6 134.3 [132.3]
2016-11-29 22:15:06,881 set_state: Restored iterator to line 41998 of 42068.
2016-11-29 22:15:06,882 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-11-29 22:15:06,882 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2016-11-29 22:15:06,883 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2016-11-29 22:15:06,884 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2016-11-29 22:15:06,884 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-11-29 22:15:06,885 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2016-11-29 22:15:06,886 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-11-29 22:15:06,886 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2016-11-29 22:15:06,889 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2016-11-29 22:15:06,890 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-11-29 22:15:06,890 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-11-29 22:15:06,891 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2016-11-29 22:15:06,892 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-11-29 22:15:06,892 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-11-29 22:15:06,893 set_state: layers/output_layer/input/b_gradient <- array(101,)
2016-11-29 22:15:06,896 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 4.
2016-11-29 22:15:06,897 _reset: Generating a random order of input lines.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 132.29.
2016-11-29 22:15:16,987 _log_update: [132] (7.4 %) of epoch 5 -- lr = 0.2, cost = 3.94, duration = 7.6 ms
2016-11-29 22:15:32,257 _log_update: [332] (18.7 %) of epoch 5 -- lr = 0.2, cost = 4.42, duration = 7.6 ms
2016-11-29 22:15:47,534 _log_update: [532] (29.9 %) of epoch 5 -- lr = 0.2, cost = 4.27, duration = 7.6 ms
2016-11-29 22:16:02,827 _log_update: [732] (41.2 %) of epoch 5 -- lr = 0.2, cost = 4.06, duration = 7.6 ms
2016-11-29 22:16:18,101 _log_update: [932] (52.4 %) of epoch 5 -- lr = 0.2, cost = 4.16, duration = 7.5 ms
2016-11-29 22:16:33,379 _log_update: [1132] (63.7 %) of epoch 5 -- lr = 0.2, cost = 4.11, duration = 7.6 ms
2016-11-29 22:16:48,641 _log_update: [1332] (74.9 %) of epoch 5 -- lr = 0.2, cost = 4.40, duration = 7.6 ms
2016-11-29 22:17:03,921 _log_update: [1532] (86.2 %) of epoch 5 -- lr = 0.2, cost = 3.98, duration = 7.5 ms
2016-11-29 22:17:19,200 _log_update: [1732] (97.4 %) of epoch 5 -- lr = 0.2, cost = 4.21, duration = 7.6 ms
2016-11-29 22:17:24,516 _validate: [1772] First validation sample, perplexity 135.25.
2016-11-29 22:17:31,548 _validate: [1775] Center of validation, perplexity 135.13.
2016-11-29 22:17:38,595 _validate: [1778] Last validation sample, perplexity 135.18.
2016-11-29 22:17:38,595 _log_validation: [1778] Validation set cost history: 169.7 141.6 134.3 [132.3] 135.2
2016-11-29 22:17:38,596 set_state: layers/output_layer/input/W <- array(256, 101)
2016-11-29 22:17:38,596 set_state: layers/output_layer/level1/b <- array(101, 100)
2016-11-29 22:17:38,597 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 22:17:38,597 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 22:17:38,598 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 22:17:38,599 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 22:17:38,602 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2016-11-29 22:17:38,603 set_state: layers/output_layer/input/b <- array(101,)
2016-11-29 22:17:38,604 _reset_state: [1775] (99.83 %) of epoch 4
2016-11-29 22:17:38,604 _log_validation: [1775] Validation set cost history: 169.7 141.6 134.3 [132.3]
2016-11-29 22:17:38,605 set_state: Restored iterator to line 41998 of 42068.
2016-11-29 22:17:38,605 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-11-29 22:17:38,605 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2016-11-29 22:17:38,607 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2016-11-29 22:17:38,607 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2016-11-29 22:17:38,608 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-11-29 22:17:38,608 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2016-11-29 22:17:38,609 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-11-29 22:17:38,609 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2016-11-29 22:17:38,612 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2016-11-29 22:17:38,613 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-11-29 22:17:38,614 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-11-29 22:17:38,614 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2016-11-29 22:17:38,615 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-11-29 22:17:38,616 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-11-29 22:17:38,616 set_state: layers/output_layer/input/b_gradient <- array(101,)
2016-11-29 22:17:38,619 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
Model performance stopped improving. Decreasing learning rate from 0.25 to 0.125 and resetting state to 100 % of epoch 4.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 132.29.
Training finished in 0 hours 17.8 minutes.
2016-11-29 22:17:38,621 set_state: layers/output_layer/input/W <- array(256, 101)
2016-11-29 22:17:38,621 set_state: layers/output_layer/level1/b <- array(101, 100)
2016-11-29 22:17:38,622 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 22:17:38,622 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 22:17:38,623 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 22:17:38,624 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 22:17:38,627 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2016-11-29 22:17:38,628 set_state: layers/output_layer/input/b <- array(101,)
Best validation set perplexity: 132.220660234
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
Restoring neural network state.
Building text scorer.
Scoring text.
Number of sentences: 3761
Number of words: 86191
Number of predicted probabilities: 82430
Cross entropy (base e): 4.833943884392687
Perplexity: 125.70575326937727
