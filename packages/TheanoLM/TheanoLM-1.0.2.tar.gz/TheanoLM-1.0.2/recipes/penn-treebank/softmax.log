THEANO_FLAGS=floatX=float32,device=gpu,nvcc.fastmath=True
/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2016-11-29 23:06:54,240 train: TRAINING OPTIONS
2016-11-29 23:06:54,240 train: sequence_length: 25
2016-11-29 23:06:54,240 train: validation_frequency: 1
2016-11-29 23:06:54,240 train: max_epochs: 15
2016-11-29 23:06:54,240 train: patience: 0
2016-11-29 23:06:54,240 train: min_epochs: 1
2016-11-29 23:06:54,240 train: stopping_criterion: no-improvement
2016-11-29 23:06:54,240 train: batch_size: 32
2016-11-29 23:06:54,240 train: max_annealing_count: 0
2016-11-29 23:06:54,240 train: OPTIMIZATION OPTIONS
2016-11-29 23:06:54,240 train: learning_rate: 1.0
2016-11-29 23:06:54,240 train: cost_function: cross-entropy
2016-11-29 23:06:54,241 train: ignore_unk: False
2016-11-29 23:06:54,241 train: gradient_decay_rate: 0.9
2016-11-29 23:06:54,241 train: unk_penalty: None
2016-11-29 23:06:54,241 train: max_gradient_norm: 5.0
2016-11-29 23:06:54,241 train: num_noise_samples: 1
2016-11-29 23:06:54,241 train: momentum: 0.9
2016-11-29 23:06:54,241 train: weights: [ 1.]
2016-11-29 23:06:54,241 train: sqr_gradient_decay_rate: 0.999
2016-11-29 23:06:54,241 train: epsilon: 1e-06
2016-11-29 23:06:54,241 train: noise_sharing: None
2016-11-29 23:06:54,241 train: method: adagrad
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2016-11-29 23:06:55,417 __init__: One epoch of training data contains 1778 mini-batch updates.
2016-11-29 23:06:55,417 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2016-11-29 23:06:55,417 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2016-11-29 23:06:55,440 _reset: Generating a random order of input lines.
Building neural network.
2016-11-29 23:06:55,447 __init__: Creating layers.
2016-11-29 23:06:55,447 __init__: - NetworkInput name=word_input inputs=[] size=10001
2016-11-29 23:06:55,447 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100
2016-11-29 23:06:55,486 __init__: - LSTMLayer name=hidden_layer inputs=[projection_layer] size=256
2016-11-29 23:06:55,604 __init__: - SoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001
2016-11-29 23:06:55,721 __init__: Initializing parameters.
2016-11-29 23:06:55,721 __init__: - layers/projection_layer/W size=1000100 type=float32
2016-11-29 23:06:55,721 __init__: - layers/hidden_layer/layer_input/W size=102400 type=float32
2016-11-29 23:06:55,721 __init__: - layers/hidden_layer/step_input/W size=262144 type=float32
2016-11-29 23:06:55,721 __init__: - layers/hidden_layer/layer_input/b size=1024 type=float32
2016-11-29 23:06:55,721 __init__: - layers/output_layer/input/W size=2560256 type=float32
2016-11-29 23:06:55,721 __init__: - layers/output_layer/input/b size=10001 type=float32
2016-11-29 23:06:55,722 __init__: Total number of parameters: 3935925
Compiling optimization function.
Building text scorer for cross-validation.
Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2016-11-29 23:07:33,760 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 5.74, duration = 12.8 ms
2016-11-29 23:07:59,566 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.67, duration = 12.8 ms
2016-11-29 23:08:25,342 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.17, duration = 12.8 ms
2016-11-29 23:08:51,093 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.19, duration = 12.8 ms
2016-11-29 23:09:16,840 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.40, duration = 12.8 ms
2016-11-29 23:09:42,593 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.15, duration = 12.8 ms
2016-11-29 23:10:08,324 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.43, duration = 12.8 ms
2016-11-29 23:10:34,046 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 4.91, duration = 12.8 ms
2016-11-29 23:11:02,890 _validate: [1772] First validation sample, perplexity 147.39.
2016-11-29 23:11:23,380 _validate: [1775] Center of validation, perplexity 147.42.
2016-11-29 23:11:43,983 _validate: [1778] Last validation sample, perplexity 147.34.
2016-11-29 23:11:44,024 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 23:11:44,025 _log_validation: [1778] Validation set cost history: [147.4]
2016-11-29 23:11:44,026 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 4.6 minutes. Best validation perplexity 147.39.
2016-11-29 23:11:46,864 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 4.81, duration = 12.8 ms
2016-11-29 23:12:12,626 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.90, duration = 12.8 ms
2016-11-29 23:12:38,373 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 4.81, duration = 12.8 ms
2016-11-29 23:13:04,184 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 4.53, duration = 12.8 ms
2016-11-29 23:13:29,952 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 4.98, duration = 12.8 ms
2016-11-29 23:13:55,694 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 4.44, duration = 12.8 ms
2016-11-29 23:14:21,446 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.98, duration = 12.8 ms
2016-11-29 23:14:47,201 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.74, duration = 12.8 ms
2016-11-29 23:15:12,955 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 4.74, duration = 12.8 ms
2016-11-29 23:15:39,010 _validate: [1772] First validation sample, perplexity 126.65.
2016-11-29 23:15:59,532 _validate: [1775] Center of validation, perplexity 126.75.
2016-11-29 23:16:20,015 _validate: [1778] Last validation sample, perplexity 126.89.
2016-11-29 23:16:20,040 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 23:16:20,040 _log_validation: [1778] Validation set cost history: 147.4 [126.7]
2016-11-29 23:16:20,042 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 4.6 minutes. Best validation perplexity 126.67.
2016-11-29 23:16:25,713 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 4.62, duration = 12.8 ms
2016-11-29 23:16:51,543 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 4.36, duration = 12.8 ms
2016-11-29 23:17:17,348 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 4.02, duration = 13.6 ms
2016-11-29 23:17:43,144 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.18, duration = 12.8 ms
2016-11-29 23:18:08,926 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 4.68, duration = 12.8 ms
2016-11-29 23:18:34,682 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 4.49, duration = 12.8 ms
2016-11-29 23:19:00,497 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 4.47, duration = 12.8 ms
2016-11-29 23:19:26,257 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.19, duration = 12.8 ms
2016-11-29 23:19:52,003 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.59, duration = 12.8 ms
2016-11-29 23:20:15,221 _validate: [1772] First validation sample, perplexity 123.89.
2016-11-29 23:20:35,715 _validate: [1775] Center of validation, perplexity 123.74.
2016-11-29 23:20:56,216 _validate: [1778] Last validation sample, perplexity 123.76.
2016-11-29 23:20:56,239 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-11-29 23:20:56,239 _log_validation: [1778] Validation set cost history: 147.4 126.7 [123.8]
2016-11-29 23:20:56,241 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 123.76.
2016-11-29 23:21:04,758 _log_update: [66] (3.7 %) of epoch 4 -- lr = 1, cost = 4.07, duration = 12.8 ms
2016-11-29 23:21:30,517 _log_update: [266] (15.0 %) of epoch 4 -- lr = 1, cost = 4.29, duration = 12.8 ms
2016-11-29 23:21:56,277 _log_update: [466] (26.2 %) of epoch 4 -- lr = 1, cost = 4.38, duration = 12.8 ms
2016-11-29 23:22:22,028 _log_update: [666] (37.5 %) of epoch 4 -- lr = 1, cost = 4.01, duration = 12.8 ms
2016-11-29 23:22:47,844 _log_update: [866] (48.7 %) of epoch 4 -- lr = 1, cost = 3.87, duration = 12.8 ms
2016-11-29 23:23:13,634 _log_update: [1066] (60.0 %) of epoch 4 -- lr = 1, cost = 4.27, duration = 12.8 ms
2016-11-29 23:23:39,457 _log_update: [1266] (71.2 %) of epoch 4 -- lr = 1, cost = 4.34, duration = 12.8 ms
2016-11-29 23:24:05,254 _log_update: [1466] (82.5 %) of epoch 4 -- lr = 1, cost = 4.39, duration = 12.8 ms
2016-11-29 23:24:31,042 _log_update: [1666] (93.7 %) of epoch 4 -- lr = 1, cost = 4.55, duration = 12.8 ms
2016-11-29 23:24:51,393 _validate: [1772] First validation sample, perplexity 126.99.
2016-11-29 23:25:11,845 _validate: [1775] Center of validation, perplexity 126.87.
2016-11-29 23:25:32,284 _validate: [1778] Last validation sample, perplexity 126.58.
2016-11-29 23:25:32,284 _log_validation: [1778] Validation set cost history: 147.4 126.7 [123.8] 126.9
2016-11-29 23:25:32,287 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-11-29 23:25:32,288 set_state: layers/output_layer/input/b <- array(10001,)
2016-11-29 23:25:32,289 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 23:25:32,289 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 23:25:32,291 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 23:25:32,291 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 23:25:32,292 _reset_state: [1775] (99.83 %) of epoch 3
2016-11-29 23:25:32,292 _log_validation: [1775] Validation set cost history: 147.4 126.7 [123.8]
2016-11-29 23:25:32,293 set_state: Restored iterator to line 41998 of 42068.
2016-11-29 23:25:32,293 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-11-29 23:25:32,294 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-11-29 23:25:32,297 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-11-29 23:25:32,298 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-11-29 23:25:32,298 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-11-29 23:25:32,298 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-11-29 23:25:32,300 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-11-29 23:25:32,303 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-11-29 23:25:32,303 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-11-29 23:25:32,304 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-11-29 23:25:32,305 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-11-29 23:25:32,306 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 3.
2016-11-29 23:25:32,308 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 123.76.
2016-11-29 23:25:43,657 _log_update: [88] (4.9 %) of epoch 4 -- lr = 0.5, cost = 4.10, duration = 12.8 ms
2016-11-29 23:26:09,456 _log_update: [288] (16.2 %) of epoch 4 -- lr = 0.5, cost = 4.12, duration = 12.8 ms
2016-11-29 23:26:35,260 _log_update: [488] (27.4 %) of epoch 4 -- lr = 0.5, cost = 3.93, duration = 12.8 ms
2016-11-29 23:27:01,058 _log_update: [688] (38.7 %) of epoch 4 -- lr = 0.5, cost = 4.39, duration = 12.8 ms
2016-11-29 23:27:26,858 _log_update: [888] (49.9 %) of epoch 4 -- lr = 0.5, cost = 4.18, duration = 12.8 ms
2016-11-29 23:27:52,672 _log_update: [1088] (61.2 %) of epoch 4 -- lr = 0.5, cost = 3.96, duration = 12.8 ms
2016-11-29 23:28:18,462 _log_update: [1288] (72.4 %) of epoch 4 -- lr = 0.5, cost = 4.21, duration = 12.8 ms
2016-11-29 23:28:44,268 _log_update: [1488] (83.7 %) of epoch 4 -- lr = 0.5, cost = 4.12, duration = 12.8 ms
2016-11-29 23:29:10,072 _log_update: [1688] (94.9 %) of epoch 4 -- lr = 0.5, cost = 4.09, duration = 12.8 ms
2016-11-29 23:29:27,603 _validate: [1772] First validation sample, perplexity 124.67.
2016-11-29 23:29:48,047 _validate: [1775] Center of validation, perplexity 124.73.
2016-11-29 23:30:08,491 _validate: [1778] Last validation sample, perplexity 124.72.
2016-11-29 23:30:08,491 _log_validation: [1778] Validation set cost history: 147.4 126.7 [123.8] 124.7
2016-11-29 23:30:08,494 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-11-29 23:30:08,495 set_state: layers/output_layer/input/b <- array(10001,)
2016-11-29 23:30:08,496 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 23:30:08,496 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 23:30:08,498 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 23:30:08,498 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-11-29 23:30:08,499 _reset_state: [1775] (99.83 %) of epoch 3
2016-11-29 23:30:08,500 _log_validation: [1775] Validation set cost history: 147.4 126.7 [123.8]
2016-11-29 23:30:08,500 set_state: Restored iterator to line 41998 of 42068.
2016-11-29 23:30:08,501 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-11-29 23:30:08,501 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-11-29 23:30:08,504 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-11-29 23:30:08,505 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-11-29 23:30:08,505 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-11-29 23:30:08,506 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-11-29 23:30:08,507 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-11-29 23:30:08,510 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-11-29 23:30:08,511 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-11-29 23:30:08,511 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-11-29 23:30:08,512 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-11-29 23:30:08,513 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 3.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 123.76.
Training finished in 0 hours 23.0 minutes.
2016-11-29 23:30:08,518 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-11-29 23:30:08,519 set_state: layers/output_layer/input/b <- array(10001,)
2016-11-29 23:30:08,519 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-11-29 23:30:08,520 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-11-29 23:30:08,521 set_state: layers/projection_layer/W <- array(10001, 100)
2016-11-29 23:30:08,521 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
Best validation set perplexity: 123.736502076
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
Restoring neural network state.
Building text scorer.
Scoring text.
Number of sentences: 3761
Number of words: 86191
Number of predicted probabilities: 82430
Cross entropy (base e): 4.776628622219357
Perplexity: 118.70348040668682
