{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction to ABC\n",
    "\n",
    "Approximate Bayesian computation\n",
    "(ABC) and so called \"likelihood free\" Markov chain Monte Carlo techniques are popular methods\n",
    "for tackling parameter inference in scenarios where the likelihood is intractable or unknown.\n",
    "These methods are called likelihood free as they are free from any assumption about the form of the likelihood, as ABC aims to simulate samples from the parameter posterior distribution directly.\n",
    "In traditional MCMC approaches the target distribution is the posterior distribution of interest and in practice our estimate of this pdf is approximate due to finite sampling time resulting in a correlated chain which we hope has converged.\n",
    "ABC methods are also approximate in the sense that samples are generated from trial distributions which we hope are close to the real posterior of interest.\n",
    "The [wikipedia]( https://en.wikipedia.org/wiki/Approximate_Bayesian_computation) page on ABC has a good introduction to the topic.\n",
    "\n",
    "The simplest ABC algorithm is rejection sampling. \n",
    "Given a set of parameters, $\\theta$, with associated priors, $\\pi(\\theta)$ and a forward simulated model for the data,\n",
    "\n",
    "$\\pi(D|\\theta)$. \n",
    "\n",
    "We can simulate from the posterior distribution, $P(\\theta|D)$, by first drawing sample parameters\n",
    "\n",
    "$\\theta^* \\sim \\pi(\\theta)$, \n",
    "\n",
    "then simulating a dataset with these parameters \n",
    "\n",
    "$D^* \\sim \\pi(D|\\theta^*)$.\n",
    "\n",
    "In a simple rejection sampling algorithm, we reject $D^*$  unless it matches the true data, $D$.\n",
    "For discrete data this algorithm would not be practical as many simulated samples would be rejected until an exact match is found.\n",
    "In practice we make an approximation and accept simulated datasets which are \"close\" to the true data. This introduces the idea of a distance metric and tolerance level in ABC. We accept proposed parameters $\\theta^*$, if \n",
    "\n",
    "$\\rho(D^* - D) <\\epsilon$\n",
    "\n",
    "where $\\rho$ is the distance metric, which could be e.g. the Euclidean norm $||D^* - D||$,  and $\\epsilon$ is a tolerance threshold. This procedure produces samples from \n",
    "\n",
    "$P(\\theta | \\rho(D^*-D)<\\epsilon)$ \n",
    "\n",
    "which will be a good approximation of the true posterior if $\\epsilon$ is small.\n",
    "\n",
    "The tolerance threshold in ABC controls which of the proposed parameters are accepted given the distance metric. There are two considerations in choosing this threshold. If the tolerance is too high then too many proposed parameters are accepted and the prior distribution dominates the results e.g. if the tolerance level is infinity then we would just recover the prior distribution from the algorithm. If the tolerance level is too low then the sampler is very inefficient with many proposed points being rejected.\n",
    "A compromise is to select a set of decreasing tolerance levels where for the initial iterations in the algorithm we accept points in parameter space which do not represent the data with high accuracy but as the algorithm progresses the tolerance level decreases and our estimate of the true posterior distribution improves.\n",
    "\n",
    "In many cases it may be simpler to work with some lower dimension summary statistic of the data, $S(D)$,\n",
    "rather then the full dataset. In this case the chosen statistic needs to be a so-called *sufficient statistic* in that\n",
    "any information about the parameter of interest which is contained in the data, is also contained in the summary statistic. More formally a statistic $S(D)$ is sufficient for $\\theta$ if the distribution $P(D|S(D))$ does not depend on $\\theta$.\n",
    "This requirement ensures that in summarizing the data we have not thrown away constraining information about $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ABC Sequential Monte Carlo\n",
    "\n",
    "Rather than drawing candiates $\\theta^*$, one at a time, we can\n",
    "speed up the ABC algorithm by working with large\n",
    "pools of candidates, called particles, simultaneously. \n",
    "At each stage of the algorithm the particles are perturbed and filtered using the distance metric, and eventually\n",
    "this pool of particles move closer and closer to simulating from the desired posterior distribution.\n",
    "This approach is known as Sequential Monte Carlo or Particle Monte Carlo sampling.\n",
    "\n",
    "Outline of the ABC SMC algorithm:\n",
    "\n",
    "- At iteration 0, for each particle:\n",
    "    1. Generate $\\theta_i^* \\sim \\pi(\\theta)$. \n",
    "    2. Simulate a dataset $D_i^* \\sim \\pi(D|\\theta_i^*)$\n",
    "    3. If  $\\rho(S(D_i^*) - S(D))<\\epsilon_0$ accept $\\theta_i^*$ else go to 1.\n",
    "- set $\\theta_{i,0} = \\theta_i^*$\n",
    "- set weights for each particle $w_{i,0} = 1/N$\n",
    "- evaluate the covariance amongst particles $\\sigma^2_{1:N;0}$\n",
    "- At iteration t>0, for each particle: \n",
    "    4. Sample random particle from previous iteration $\\theta_i^* \\sim \\theta_{1:N;0}$ \n",
    "    5. Perturb $\\theta_i^*$ by drawing $\\theta_i^{**} \\sim \\mathcal{N}(\\theta^*, \\sigma^2_{t-1})$\n",
    "    6. Simulate a dataset $D_i^* \\sim \\pi(D|\\theta_i^{**})$\n",
    "    7. If  $\\rho(S(D_i^*) - S(D))<\\epsilon_0$ accept $\\theta_i^{**}$ else go to 4.\n",
    "- set $\\theta_{i,t} = \\theta_i^{**}$\n",
    "- set weights for each particle $w_{i,t}$ using a transition kernel\n",
    "- evaluate the covariance amongst particles $\\sigma^2_{1:N;t}$\n",
    "    \n",
    "Different ABC SMC algorithms can be distinguished\n",
    "by how sampling weights are assigned to the particles in the pool.\n",
    "In order to perturb and filter the particles we need a transition kernel. The transition\n",
    "kernel serves the same purpose as the proposal distribution in a standard\n",
    "MCMC algorithm. The transition kernel specifies the distribution of a random variable that will\n",
    "be added to each particle to move it around in the parameter space.\n",
    "For more details on this please see \n",
    "[Beaumont et al 2009]( https://arxiv.org/abs/0805.2256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
