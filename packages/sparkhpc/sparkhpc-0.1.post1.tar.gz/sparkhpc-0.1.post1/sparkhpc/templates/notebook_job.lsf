#!/bin/sh
#BSUB -J spark_notebook
#BSUB -W 01:00 # requesting one hour 
#BSUB -n 10 # requesting 10 cores for the driver

# spark_scripts is assumed here to be a link to the spark-on-hpc-clusters repo
source $HOME/spark-scripts/setup_spark.sh

# the next line assumes that you have 2 Gb/core -- it gives 70% of the available memory to the JVM that runs the driver
DRIVER_MEM=$(printf '%.f' $(echo "$LSB_DJOB_NUMPROC * 2 * .7" | bc))G

# if you want to specify other options for spark, add them to the spark_options string 
# to specify a spark configuration directory, use the --spark_config flag 
/cluster/apps/spark/scripts/start_notebook.py --spark --spark_options "--master yarn --driver-memory ${DRIVER_MEM}" 
